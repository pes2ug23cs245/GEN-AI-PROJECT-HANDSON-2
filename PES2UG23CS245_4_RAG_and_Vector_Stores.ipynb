{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "411ce1c2",
      "metadata": {
        "id": "411ce1c2"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace94db8",
      "metadata": {
        "id": "ace94db8"
      },
      "source": [
        "# Unit 2: RAG, Vector Stores, and Indexing\n",
        "\n",
        "## Introduction\n",
        "LLMs have a knowledge cutoff and can hallucinate. **Retrieval Augmented Generation (RAG)** solves this by retrieving relevant data and injecting it into the prompt.\n",
        "\n",
        "In this notebook, we will master:\n",
        "1.  **Embeddings:** Representing text as vectors.\n",
        "2.  **Vector Stores:** Storing and searching vectors (FAISS).\n",
        "3.  **Na\u00efve RAG:** The standard Retrieval -> Augment -> Generate pipeline.\n",
        "4.  **Indexing Challenges:** Deep dive into how vector databases search efficiently (Flat, IVF, HNSW, PQ).\n",
        "\n",
        "---\n",
        "\n",
        "## Part 4a: Embeddings & Vector Space\n",
        "\n",
        "### 1. Introduction: Computers Don't Read English\n",
        "\n",
        "If you ask a computer \"Is a cat similar to a dog?\", it doesn't know. To a computer, \"cat\" is just a string of bytes: `01100011...`.\n",
        "\n",
        "To solve this, we use **Embeddings**.\n",
        "\n",
        "### What is an Embedding?\n",
        "An embedding is a translation from **Words** to **Lists of Numbers (Vectors)**, such that similar words represent close numbers.\n",
        "\n",
        "### The Process (Flowchart)\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[\"Input Text ('Apple')\"] -->|Tokenization| B[\"Tokens (101, 255)\"]\n",
        "    B -->|Embedding Model| C[\"Vector List ([0.1, -0.5, 0.9...])\"]\n",
        "    C -->|Store| D[\"Vector Database\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2bd7eb17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629,
          "referenced_widgets": [
            "c3345bbe14b6449886439a2f4a540802",
            "736f2825068c4a618fdb42a2d8f4e0bf",
            "e6fc06ac3a644e048b27ae3f848335aa",
            "1e9789376ad8498da65d6b8376add94d",
            "4f5a634c4ff0496a808c8fbc7b9712af",
            "8682c9518e1648e1983fdc7cfdfe5402",
            "980b589c9275487d9c15c7d764c4ab08",
            "5b9fad3c293e4b6a8f66dc2ff7406728",
            "344e23fd77c448738c3081bd7154fc70",
            "cfa0d34610f04e0caea8ad0705e6d906",
            "9bb60f3e14794e89897f1aeb737ed8d4",
            "67f2e8664ab74405a08013be7efc4340",
            "cb5fc90c629b49b2848efb80f6bbaac4",
            "da92c1daf87f4e848d9a9ba984805342",
            "b4ea526afa5f42a69657665aec5a6c8e",
            "40d116bc781c4b96a341b3ae58cba250",
            "19b550cdf2934330865fc20d956d3b23",
            "068d8474413e46f3a739bd08f3cd3c9b",
            "5c124cabe2f84e41960b3818a915bd39",
            "500c057ca96a479a846b00606dd78764",
            "032438b26f484e8fa37ef356dd13abbc",
            "ef958c5b2e074ede916d4febc052ebe5",
            "c0743aa6e1ed4523897cee12f36d6cdb",
            "8e7920807a884a788c15d82e15d29ad1",
            "324e48516eee4b368b5496ef90ff60ae",
            "d6c2b65f7be14735bb842cf9a4234d90",
            "95c1326604c74f508fac3ac3bae1d0b6",
            "37f12d2e667a496ebaea2bcb14b51aa7",
            "639888cb0b3144bc8cbb0a0358c8ccd6",
            "69515545b4574106a7a2ea04dfde11ea",
            "d0869e3605334f58b06b7cc815341bef",
            "3c0e4e97118b463ca99f63bbf69da222",
            "5ef0ef68eabc4707b2b3efa02da153d9",
            "c44ca06a8ee8404cb422fe8847618cc7",
            "a0d37d5562cc4bd58e5265967f12b3d9",
            "f1257e22521e480b9d97731d6b22d17e",
            "37e5c5f3ffab4171ba9ad16a0990a688",
            "43589eddddd941dab1826162d87127ac",
            "6052ce98bc104ab9a1eeb4fed484313e",
            "43bffcae4e08491ca1327a09081b57f5",
            "077f75c63ad045c8a1d3307567ad2555",
            "9c24426deb2d46328ccbba05e1e69244",
            "f374c29cb0a54ca28cbab4d278f351cd",
            "897692bd5f434f0ab6c428b8f47fc230",
            "f488c3ddf9ea4323ac2391ed7b2abd80",
            "6bd79ad87bb24622af32ae1aeaa8fe5f",
            "54e7a4dea4d741dcbdd5f5a698b1a376",
            "8da2153ace324b6ca4affbf52283952e",
            "5f20e7adf2684e6d88184b1139e13405",
            "03ad8b193e5949cca1fcd8dfd6ca8721",
            "ee7b20d592c1423d914fbc672fc18b72",
            "10764c9689f143819f70dc0e95f8b9e8",
            "5a8ad670afda464d8df1d6e8afc8c0c5",
            "6d7f6f13c79a4507b5f56d4706e08d4b",
            "f8c536e171e545e5807da1edb2b5e9c5",
            "5b554aec2777477e9da0176462407bb6",
            "2bbbe9610a29440a80c9989000be88b0",
            "5266b780242646d5ae3acd6d8cbfc95d",
            "6349c3372b404af6a63f793cef2b186b",
            "442cb638182c4d3d9a055edbc4256676",
            "966e5510ae9d40e1b87a259f86d239ce",
            "1c68d47a1a474eafb665c4e61adb0322",
            "0c9159d6c2dc448fa2c13c8c289a00fe",
            "48c1bb46b31b432ca792f0b437bf0436",
            "587e8a5493a54f4b89a0d12548cbbd59",
            "3319c466f78149ba86d000446338b98e",
            "144cca7dc1b247e5ae72ecabf6329fb8",
            "2cb16b4130504c32b708a2f351f9653f",
            "54f8b7a81e7c422da0f9719bd5b2aa2c",
            "03f870d8fa7d445da9f172fca38bffac",
            "a2693d86cab7447799f6450839239d9c",
            "956c46c07afc42cfb63bed0f0e47c205",
            "df212e68832c42c689523aa4263d2dc0",
            "7f666fcd707d4d00821e97358bd8f286",
            "212382a4e018466bb12d0a5a37dddbd5",
            "9ff3cd5e131c4a9687b9e9b952d85e05",
            "cf7c9b8b29494c7590f9d4f641b32de8",
            "0d72cd7d28f342e6a440a6d40d471d15",
            "514414496de943cbbdad19683640d0b8",
            "f8368d26c90343d9a8b8d52dd0ddef82",
            "82d02859db0f4eebad7cbbb77a8864b0",
            "a4aff3da939a4608962cfaa893673b7c",
            "6ecc68c44d174a4e81c44ad7bb413e72",
            "dd317f5ce44d423cb6656e3afa76bca7",
            "c33f284df099434391eb3fb6c285d89d",
            "8cdb3f7dd7a941f8b1ea8d39f9109672",
            "28ae6925338d4812b3583cd098c9a83c",
            "fd2a5f0f73bc46918f55733f715ff239",
            "f86388344bda46e0b8d06826f0cf0fd1",
            "a44e6aa63e024c459c19b72bc3a301ac",
            "a95898757124464383fde60a83886799",
            "ee3c75e8532243cebf962587707bfafc",
            "e5cca22c8f4042c7b1f6ff56efac3830",
            "332df445b1af48ae8c96ce1da2ac80af",
            "a41a4afe3efb4376812db8f2534d39dc",
            "b750c6daabf64218a52ab5d100b3f064",
            "a4e3c1b45f8f430284bb6f70cb2b85b8",
            "50b85a983f1244c89a056e10be2f022c",
            "ce80b18836804f4dbfba4607c6213334",
            "98e55b5d4af743ffabad94a726ac4877",
            "40729d49ef4a48138268ff9bd656d350",
            "5df8fc7ae3f94adbb8d5ae28d825974d",
            "ce3b270117ea444f8d99f6c24c2997cd",
            "d83c3bc4aa174cad8bdbc89245212a19",
            "18f224ee4f9a456298eb08f4f95f0f08",
            "ddaf6dd35d814352b4a751f92a1bbed3",
            "111fa496b04f45f882cad2d84d456366",
            "1e58a16b84d84be786ca14b746ebe816",
            "4ed5281148a04989bbea759f9dee4baf",
            "976de206ce69489fbf4fbeebfe64ca1f",
            "7dc27f9f02d943ba9d5eb94c5878d669",
            "11d7d51f03a24a8cb232d895f460d14c",
            "aee725254cc7482ea86fe04ad1c066c1",
            "2fffb5c5f9844e53998cfec0f091efe0",
            "8d933656dbf7401495a056f3dd561195",
            "24ca212209304533a8793863d55a4ac6",
            "da4ae8d6235744e6979f00268f1dbf6a",
            "9aea1d154c4b4676b11a015cb8ce16a3",
            "5e550920be49410ca04c5a6de0548cf7",
            "3dc9e869d3904bdfa6a07641d2e8251a",
            "905b40300858422bbf81348eadaa6843"
          ]
        },
        "id": "2bd7eb17",
        "outputId": "57fce686-da98-4c2e-f141-3938c2422707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3345bbe14b6449886439a2f4a540802"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67f2e8664ab74405a08013be7efc4340"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0743aa6e1ed4523897cee12f36d6cdb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c44ca06a8ee8404cb422fe8847618cc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f488c3ddf9ea4323ac2391ed7b2abd80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b554aec2777477e9da0176462407bb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "144cca7dc1b247e5ae72ecabf6329fb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d72cd7d28f342e6a440a6d40d471d15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f86388344bda46e0b8d06826f0cf0fd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98e55b5d4af743ffabad94a726ac4877"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dc27f9f02d943ba9d5eb94c5878d669"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-huggingface sentence-transformers langchain-community\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Using a FREE, open-source model from Hugging Face\n",
        "# 'all-MiniLM-L6-v2' is small, fast, and very good for English.\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718acb1f",
      "metadata": {
        "id": "718acb1f"
      },
      "source": [
        "## 2. Viewing a Vector\n",
        "\n",
        "Let's see what the word \"Apple\" looks like to the machine.\n",
        "\n",
        "### Conceptual Note: Dimensions\n",
        "The vector below has **384 dimensions** (for MiniLM).\n",
        "- Imagine a graph with X and Y axes (2 Dimensions). You can plot a point (x, y).\n",
        "- Now imagine adding Z (3 Dimensions).\n",
        "- Now imagine **384 axes**.\n",
        "\n",
        "Each axis represents a feature (e.g., \"Is it a fruit?\", \"Is it red?\", \"Is it tech-related?\"). The numbers aren't random; they encode meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6c67eb1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c67eb1c",
        "outputId": "5b6d0c9f-fe8e-464d-d21a-ca97c5770c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensionality: 384\n",
            "First 5 numbers: [-0.006138487718999386, 0.03101177327334881, 0.06479360908269882, 0.01094149798154831, 0.005267191678285599]\n"
          ]
        }
      ],
      "source": [
        "vector = embeddings.embed_query(\"Apple\")\n",
        "\n",
        "print(f\"Dimensionality: {len(vector)}\")\n",
        "print(f\"First 5 numbers: {vector[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da0fd791",
      "metadata": {
        "id": "da0fd791"
      },
      "source": [
        "## 3. The Math: Cosine Similarity\n",
        "\n",
        "How do we know if two vectors are close? We measure the **Angle** between them.\n",
        "\n",
        "### Cosine Similarity Formula\n",
        "$$ \\text{Similarity} = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} $$\n",
        "\n",
        "- **1.0**: Arrows point in the Exact Same Direction (Identical).\n",
        "- **0.0**: Arrows are Perpendicular (Unrelated).\n",
        "- **-1.0**: Arrows point in Opposite Directions (Opposite).\n",
        "\n",
        "**Experiment:** Let's compare \"Cat\", \"Dog\", and \"Car\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "58dd1396",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58dd1396",
        "outputId": "0de073bc-1463-4a7a-ae00-24e1b163e03c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cat vs Dog: 0.6606\n",
            "Cat vs Car: 0.4633\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "vec_cat = embeddings.embed_query(\"Cat\")\n",
        "vec_dog = embeddings.embed_query(\"Dog\")\n",
        "vec_car = embeddings.embed_query(\"Car\")\n",
        "\n",
        "print(f\"Cat vs Dog: {cosine_similarity(vec_cat, vec_dog):.4f}\")\n",
        "print(f\"Cat vs Car: {cosine_similarity(vec_cat, vec_car):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a90b5f7",
      "metadata": {
        "id": "5a90b5f7"
      },
      "source": [
        "### Analysis\n",
        "You should see that **Cat & Dog** score higher (e.g., ~0.8) than **Cat & Car** (e.g., ~0.3).\n",
        "This Mathematical Distance is the foundation of all Search engines and RAG systems.\n",
        "\n",
        "This is arguably the most important concept in modern AI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6278ad2",
      "metadata": {
        "id": "f6278ad2"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1865ce8f",
      "metadata": {
        "id": "1865ce8f"
      },
      "source": [
        "# Unit 2 - Part 4b: Naive RAG Pipeline\n",
        "\n",
        "## 1. Introduction: The Open-Book Test\n",
        "\n",
        "RAG (Retrieval-Augmented Generation) is just an Open-Book Test architecture.\n",
        "1.  **Retrieval:** Find the right page in the textbook.\n",
        "2.  **Generation:** Write the answer using that page.\n",
        "\n",
        "### The Pipeline (Flowchart)\n",
        "```mermaid\n",
        "graph TD\n",
        "    User[User Question] --> Retriever[Retriever System]\n",
        "    Retriever -->|Search Database| Docs[Relevant Documents]\n",
        "    Docs --> Combiner[Prompt Template]\n",
        "    User --> Combiner\n",
        "    Combiner -->|Full Prompt w/ Context| LLM[Gemini Model]\n",
        "    LLM --> Answer[Final Answer]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4dcd7e45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dcd7e45",
        "outputId": "20eb8c3c-e78e-4685-c980-7b3debebf26e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/66.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your Google API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "!pip install -qU langchain-google-genai\n",
        "%pip install python-dotenv --upgrade --quiet faiss-cpu langchain-huggingface sentence-transformers langchain-community\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "# Using the same free model as Part 4a\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6f457ce",
      "metadata": {
        "id": "b6f457ce"
      },
      "source": [
        "## 2. The \"Knowledge Base\" (Grounding)\n",
        "\n",
        "LLMs hallucinate because they rely on \"parametric memory\" (what they learned during training).\n",
        "RAG introduces \"non-parametric memory\" (external facts).\n",
        "\n",
        "Let's define some facts the LLM definitely *does not* know."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "234faff9",
      "metadata": {
        "id": "234faff9"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=\"Piyush's favorite food is Pizza with extra cheese.\"),\n",
        "    Document(page_content=\"The secret password to the lab is 'Blueberry'.\"),\n",
        "    Document(page_content=\"LangChain is a framework for developing applications powered by language models.\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5812c6",
      "metadata": {
        "id": "2c5812c6"
      },
      "source": [
        "## 3. Indexing ( Storing the knowledge)\n",
        "\n",
        "We use **FAISS** (Facebook AI Similarity Search) to store the embeddings.\n",
        "Think of FAISS as a super-fast librarian that organizes books by content, not title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c1e1581d",
      "metadata": {
        "id": "c1e1581d"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5d826cf",
      "metadata": {
        "id": "d5d826cf"
      },
      "source": [
        "## 4. The RAG Chain\n",
        "\n",
        "We use LCEL to stitch it together.\n",
        "\n",
        "**Step 1:** The `retriever` takes the question, converts it to numbers, and finds the closest document.\n",
        "**Step 2:** `RunnablePassthrough` holds the question.\n",
        "**Step 3:** The `prompt` combines them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "67a7a5ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67a7a5ab",
        "outputId": "6b892c45-4665-43fd-9a78-8bba74974336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The secret password to the lab is 'Blueberry'.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"\"\"\n",
        "Answer based ONLY on the context below:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "result = chain.invoke(\"What is the secret password?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c34d42",
      "metadata": {
        "id": "d4c34d42"
      },
      "source": [
        "## 5. Analysis\n",
        "\n",
        "The retrieval step is opaque here. In the next notebook (**4c**), we will look *inside* the retriever to understand how FAISS actually finds that document among millions of others."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b988faa1",
      "metadata": {
        "id": "b988faa1"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "764e4fe9",
      "metadata": {
        "id": "764e4fe9"
      },
      "source": [
        "# Unit 2 - Part 4c: Deep Dive into Indexing Algorithms\n",
        "\n",
        "## 1. Introduction: The Scale Problem\n",
        "\n",
        "Comparing 1 vector against 10 vectors is fast.\n",
        "Comparing 1 vector against **100 Million** vectors is slow.\n",
        "\n",
        "**FAISS (Facebook AI Similarity Search)** was built to solve this.\n",
        "\n",
        "### The Trade-off Triangle\n",
        "You can pick 2:\n",
        "- **Speed** (Query time)\n",
        "- **Accuracy** (Recall)\n",
        "- **Memory** (RAM usage)\n",
        "\n",
        "We will explore algorithms that optimize different corners of this triangle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cc999db8",
      "metadata": {
        "id": "cc999db8"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Mock Data: 10,000 vectors of size 128\n",
        "d = 128\n",
        "nb = 10000\n",
        "xb = np.random.random((nb, d)).astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0c4b504",
      "metadata": {
        "id": "f0c4b504"
      },
      "source": [
        "## 2. Flat Index (Brute Force)\n",
        "\n",
        "**Concept:** Check every single item.\n",
        "\n",
        "- **Algo:** `IndexFlatL2`\n",
        "- **Pros:** 100% Accuracy (Gold Standard).\n",
        "- **Cons:** Slow (O(N)). Unusable at 1M+ vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "847ac87f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "847ac87f",
        "outputId": "05fcabc4-9494-44e6-8998-6d5bd6bc0268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flat Index contains 10000 vectors\n"
          ]
        }
      ],
      "source": [
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(xb)\n",
        "print(f\"Flat Index contains {index.ntotal} vectors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "507e1ffc",
      "metadata": {
        "id": "507e1ffc"
      },
      "source": [
        "## 3. IVF (Inverted File Index)\n",
        "\n",
        "**Concept:** Clustering / Partitioning.\n",
        "\n",
        "Imagine looking for a book. Instead of checking every shelf, you go to the \"Sci-Fi\" section. Then you only search books *in that section*.\n",
        "\n",
        "### How it works (Flowchart)\n",
        "```mermaid\n",
        "graph TD\n",
        "    Data[All 1M Vectors] -->|Train| Clusters[1000 Cluster Centers (Centroids)]\n",
        "    Query[User Query] -->|Step 1| FindClosest[Find Closest Centroid]\n",
        "    FindClosest -->|Step 2| Search[Search ONLY vectors in that Cluster]\n",
        "```\n",
        "\n",
        "**Analogy:** Voronoi Cells (Zip Codes). We only search the local zip code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "402817e5",
      "metadata": {
        "id": "402817e5"
      },
      "outputs": [],
      "source": [
        "nlist = 100 # How many 'zip codes' (clusters) we want\n",
        "quantizer = faiss.IndexFlatL2(d) # The calculator for distance\n",
        "index_ivf = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
        "\n",
        "# We MUST train it first so it learns where the clusters are\n",
        "index_ivf.train(xb)\n",
        "index_ivf.add(xb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0a28c5",
      "metadata": {
        "id": "4c0a28c5"
      },
      "source": [
        "## 4. HNSW (Hierarchical Navigable Small World)\n",
        "\n",
        "**Concept:** Six Degrees of Separation.\n",
        "\n",
        "Most data is connected. HNSW builds a **Graph**.\n",
        "- **Layer 0:** Every point connects to neighbors.\n",
        "- **Layer 1:** \"Express Highways\" connecting distant points.\n",
        "\n",
        "**Analogy:** Catching a flight.\n",
        "You don't fly Local -> Local -> Local.\n",
        "You fly Local -> **HUB** (Chicago) -> **HUB** (London) -> Local.\n",
        "\n",
        "- **Pros:** Extremely fast retrieval.\n",
        "- **Cons:** Heavier on RAM (needs to store the edges of the graph)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "42c18025",
      "metadata": {
        "id": "42c18025"
      },
      "outputs": [],
      "source": [
        "M = 16 # Number of connections per node (The 'Hub' factor)\n",
        "index_hnsw = faiss.IndexHNSWFlat(d, M)\n",
        "index_hnsw.add(xb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "645d63c1",
      "metadata": {
        "id": "645d63c1"
      },
      "source": [
        "## 5. PQ (Product Quantization)\n",
        "\n",
        "**Concept:** Compression (Lossy).\n",
        "\n",
        "Do we need 32-bit float precision (`0.123456789`)? No. `0.12` is fine.\n",
        "PQ breaks the vector into chunks and approximates them.\n",
        "\n",
        "**Analogy:** 4K Video vs 480p Video.\n",
        "- 480p is blurry, but it's 10x smaller and faster to stream.\n",
        "- Use PQ when you are RAM constrained (e.g., storing 1 Billion vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "598642ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "598642ac",
        "outputId": "7e95ef90-4f8b-4447-f612-89514387bb78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PQ Compression complete. RAM usage minimized.\n"
          ]
        }
      ],
      "source": [
        "m = 8 # Split vector into 8 sub-vectors\n",
        "index_pq = faiss.IndexPQ(d, m, 8)\n",
        "index_pq.train(xb)\n",
        "index_pq.add(xb)\n",
        "print(\"PQ Compression complete. RAM usage minimized.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}